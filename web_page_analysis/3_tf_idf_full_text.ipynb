{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. TF-IDF преобразование нормализованных текстов веб-страниц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(corpus, top_word_count = 5):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    top_words = [w[0] for w in words_freq[:top_word_count]]\n",
    "    return top_words\n",
    "\n",
    "def add_group_feature(group_id, tf_idf_feature_count=15):\n",
    "    warnings.filterwarnings('ignore')\n",
    "    group = df.loc[df['group_id']==group_id]\n",
    "    top_words = get_top_words(group['title'])\n",
    "    group['text'] = ''\n",
    "    for index, row in group.iterrows():\n",
    "        file_name = os.path.join('fulltext', f'{row[\"doc_id\"]}.txt')\n",
    "        if os.path.isfile(file_name):\n",
    "            with open(file_name) as f:\n",
    "                group['text'][index] = f.read()\n",
    "    warnings.filterwarnings('default')\n",
    "    # расстояния до ближайших соседей\n",
    "    knn = NearestNeighbors(metric='cosine')\n",
    "    X = TfidfVectorizer().fit_transform(group['text']) #ngram_range=(1, 2)\n",
    "    svd = TruncatedSVD(n_components=len(group.index))\n",
    "    X = svd.fit_transform(X)\n",
    "    knn.fit(X)\n",
    "    distances = knn.kneighbors(n_neighbors=tf_idf_feature_count)[0]\n",
    "    for j in range(tf_idf_feature_count):\n",
    "        df.loc[df['group_id'] == group_id, f'full_tfidf_{j}'] = distances[:, j]\n",
    "    # параметры длины страницы в словах\n",
    "    warnings.filterwarnings('ignore')\n",
    "    group['len_word'] = list(map(lambda x: len(x.split()), group['text']))\n",
    "    warnings.filterwarnings('default')\n",
    "    df.loc[df['group_id'] == group_id, 'text_len_word'] = group['len_word']\n",
    "    df.loc[df['group_id'] == group_id, 'text_len_word_mean'] = group['len_word'].mean()\n",
    "    df.loc[df['group_id'] == group_id, 'text_len_word_var'] = group['len_word'].var()\n",
    "    # tf-idf документа для топ-5 слов в текстах\n",
    "    warnings.filterwarnings('ignore')\n",
    "    for i in range(len(top_words)):\n",
    "        group[f'jaf{i}'] = np.zeros(len(group.index))\n",
    "    warnings.filterwarnings('default')\n",
    "    for index, row in group.iterrows():\n",
    "        tfidf = TfidfVectorizer(min_df=1)\n",
    "        try:\n",
    "            vec = tfidf.fit_transform(row['text'].split())\n",
    "            features = dict(zip(tfidf.get_feature_names(), tfidf.idf_))\n",
    "            warnings.filterwarnings('ignore')\n",
    "            for i in range(len(top_words)):\n",
    "                group[f'jaf{i}'][index] = features[top_words[i]] if top_words[i] in features else 0.\n",
    "            warnings.filterwarnings('default')\n",
    "        except ValueError:\n",
    "            pass\n",
    "    warnings.filterwarnings('ignore')\n",
    "    for i in range(len(top_words)):\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(group[f'jaf{i}'].values.reshape(-1, 1))\n",
    "        group[f'jaf{i}'] = scaler.transform(group[f'jaf{i}'].values.reshape(-1, 1))\n",
    "        df.loc[df['group_id'] == group_id, f'jaf{i}'] = group[f'jaf{i}']\n",
    "    warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_preprocessed.csv').fillna('')\n",
    "for i in tqdm(df['group_id'].unique()):\n",
    "    add_group_feature(i)\n",
    "df.to_csv('train_fulltext2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('test_preprocessed.csv').fillna('')\n",
    "for i in tqdm(df['group_id'].unique()):\n",
    "    add_group_feature(i)\n",
    "df.to_csv('test_fulltext2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
